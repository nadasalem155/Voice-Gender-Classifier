import streamlit as st
import tempfile
import os
import numpy as np
import librosa
import tensorflow as tf
import matplotlib.pyplot as plt
from audio_recorder_streamlit import audio_recorder
import time

# Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Streamlit
st.set_page_config(
    page_title="Voice Gender Recognition",
    page_icon="ğŸ™ï¸",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- Load Keras model once ---
@st.cache_resource
def load_model():
    try:
        model = tf.keras.models.load_model("gender_voice_model.keras", compile=False)
        return model
    except Exception as e:
        st.error(f"âŒ Failed to load model: {e}")
        return None

model = load_model()

# --- Audio preprocessing ---
def preprocess_audio(filename, max_len=48000):
    try:
        wav, sr = librosa.load(filename, sr=16000, mono=True)
        if len(wav) > max_len:
            wav = wav[:max_len]
        else:
            wav = np.pad(wav, (0, max_len - len(wav)))
        spec = np.abs(librosa.stft(wav, n_fft=512, hop_length=256))
        spec = np.expand_dims(spec, -1)
        spec = tf.image.resize(spec, [128, 128])
        spec = np.expand_dims(spec, 0)
        return spec, wav, sr
    except Exception as e:
        st.error(f"âš  Error processing audio: {e}")
        return None, None, None

# --- Gender prediction ---
def predict_gender(file_path):
    if model is None:
        st.error("âŒ Model not available. Please check if the model file exists.")
        return None
        
    with st.spinner("ğŸ§ Analyzing your voice... Please wait â³"):
        features, _, _ = preprocess_audio(file_path)
        if features is None:
            return None
        pred = model.predict(features, verbose=0)[0][0]
    return "ğŸ‘¨ Male" if pred > 0.5 else "ğŸ‘© Female"

# --- Initialize session state ---
def init_session_state():
    keys = [
        "uploaded_path", "recorded_path", 
        "uploaded_result", "recorded_result",
        "current_file", "audio_bytes_processed"
    ]
    for key in keys:
        if key not in st.session_state:
            st.session_state[key] = None

init_session_state()

# --- Cleanup function ---
def cleanup_file(file_path):
    if file_path and os.path.exists(file_path):
        try:
            os.remove(file_path)
            return True
        except:
            return False
    return True

# --- UI Header ---
st.title("ğŸ™ï¸ Voice Gender Recognition")
st.markdown("Upload or record your voice to detect **Male ğŸ‘¨** or **Female ğŸ‘©** using a CNN model.")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¨ÙˆÙŠØ¨
tab1, tab2 = st.tabs(["ğŸ“‚ Upload Audio", "ğŸ¤ Record Voice"])

with tab1:
    st.subheader("Upload Audio File")
    
    uploaded_file = st.file_uploader(
        "Choose a .wav, .mp3, or .ogg file:",
        type=["wav", "mp3", "ogg"],
        key="upload_widget"
    )

    if uploaded_file is not None:
        if st.session_state.recorded_path:
            cleanup_file(st.session_state.recorded_path)
            st.session_state.recorded_path = None
            st.session_state.recorded_result = None

        if st.session_state.current_file != uploaded_file.name:
            temp_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.read())
            
            st.session_state.uploaded_path = temp_path
            st.session_state.current_file = uploaded_file.name
            st.session_state.uploaded_result = predict_gender(temp_path)

    if st.session_state.uploaded_path and st.session_state.uploaded_result:
        st.success(f"**Prediction (Uploaded):** {st.session_state.uploaded_result}")

        if os.path.exists(st.session_state.uploaded_path):
            spec, wav, sr = preprocess_audio(st.session_state.uploaded_path)
            if wav is not None:
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    fig, ax = plt.subplots(figsize=(8, 2))
                    ax.plot(wav, color="#1f77b4")
                    ax.set_title("Waveform (Uploaded)")
                    ax.set_xlabel("Samples")
                    ax.set_ylabel("Amplitude")
                    plt.tight_layout()
                    st.pyplot(fig)
                
                with col2:
                    st.audio(st.session_state.uploaded_path, format="audio/wav")

        if st.button("ğŸ—‘ Remove Uploaded File", key="btn_remove_upload"):
            if cleanup_file(st.session_state.uploaded_path):
                st.session_state.uploaded_path = None
                st.session_state.uploaded_result = None
                st.session_state.current_file = None
                st.success("âœ… Uploaded file removed successfully!")
                time.sleep(0.5)
                st.rerun()

with tab2:
    st.subheader("Record Your Voice")
    st.markdown("Click the mic and speak for **2-5 seconds** ğŸ•")

    audio_bytes = audio_recorder(
        text="ğŸ™ï¸ Start Recording",
        recording_color="#e74c3c",
        neutral_color="#2c3e50",
        icon_name="microphone",
        icon_size="2x",
        key="record_widget"
    )

    if audio_bytes and st.session_state.audio_bytes_processed != audio_bytes:
        if st.session_state.uploaded_path:
            cleanup_file(st.session_state.uploaded_path)
            st.session_state.uploaded_path = None
            st.session_state.uploaded_result = None

        temp_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
        with open(temp_path, "wb") as f:
            f.write(audio_bytes)
        
        st.session_state.recorded_path = temp_path
        st.session_state.audio_bytes_processed = audio_bytes
        st.session_state.recorded_result = predict_gender(temp_path)

    if st.session_state.recorded_path and st.session_state.recorded_result:
        st.success(f"**Prediction (Recorded):** {st.session_state.recorded_result}")

        if os.path.exists(st.session_state.recorded_path):
            spec, wav, sr = preprocess_audio(st.session_state.recorded_path)
            if wav is not None:
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    fig, ax = plt.subplots(figsize=(8, 2))
                    ax.plot(wav, color="#ff7f0e")
                    ax.set_title("Waveform (Recorded)")
                    ax.set_xlabel("Samples")
                    ax.set_ylabel("Amplitude")
                    plt.tight_layout()
                    st.pyplot(fig)
                
                with col2:
                    st.audio(st.session_state.recorded_path, format="audio/wav")

        if st.button("ğŸ—‘ Remove Recorded Audio", key="btn_remove_record"):
            if cleanup_file(st.session_state.recorded_path):
                st.session_state.recorded_path = None
                st.session_state.recorded_result = None
                st.session_state.audio_bytes_processed = None
                st.success("âœ… Recording removed successfully!")
                time.sleep(0.5)
                st.rerun()

# Ø²Ø± Ù…Ø³Ø­ Ø§Ù„ÙƒÙ„
st.markdown("---")
if st.button("ğŸ—‘ Clear All Files", type="secondary", key="btn_clear_all"):
    cleanup_file(st.session_state.uploaded_path)
    cleanup_file(st.session_state.recorded_path)
    
    st.session_state.uploaded_path = None
    st.session_state.recorded_path = None
    st.session_state.uploaded_result = None
    st.session_state.recorded_result = None
    st.session_state.current_file = None
    st.session_state.audio_bytes_processed = None
    
    st.success("âœ… All files cleared successfully!")
    time.sleep(0.5)
    st.rerun()

# Ø§Ù„ÙÙˆØªØ±
st.markdown("---")
st.caption("ğŸ’¡ Powered by Streamlit â€¢ ğŸ§  Model: CNN trained on STFT Spectrograms")

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø±ÙˆØ¬
import atexit
atexit.register(lambda: [
    cleanup_file(st.session_state.uploaded_path),
    cleanup_file(st.session_state.recorded_path)
])